{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0: Qualitative Verification of Introspection in Gemma 3 IT\n",
    "\n",
    "Before building full SAE feature analysis infrastructure, we need to verify that Gemma 3 shows introspection signal similar to what we observed in Qwen models.\n",
    "\n",
    "**Goal:** Run injection + control trials and manually assess whether the model can detect injected concepts.\n",
    "\n",
    "**Go/no-go criteria:**\n",
    "- If clear signal (model reports detecting something, names concept): proceed to Phase 1\n",
    "- If weak/no signal: try larger model before abandoning\n",
    "- If high false positives: adjust prompt or injection strength\n",
    "\n",
    "**Model options:**\n",
    "- Local testing: `google/gemma-3-270m-it` (layers 5, 9, 12, 15)\n",
    "- Colab/A100: `google/gemma-3-4b-it` (layers 7, 13, 17, 22, 27, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  8 04:05:33 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sae-lens in /usr/local/lib/python3.12/dist-packages (6.36.0)\n",
      "Requirement already satisfied: transformer-lens in /usr/local/lib/python3.12/dist-packages (2.17.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
      "Requirement already satisfied: babe<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (0.0.7)\n",
      "Requirement already satisfied: datasets>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (4.0.0)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (3.9.1)\n",
      "Requirement already satisfied: plotly>=5.19.0 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (5.24.1)\n",
      "Requirement already satisfied: plotly-express>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (0.4.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (6.0.3)\n",
      "Requirement already satisfied: safetensors<1.0.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (0.7.0)\n",
      "Requirement already satisfied: simple-parsing<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (0.1.8)\n",
      "Requirement already satisfied: tenacity>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (9.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.1 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (4.57.6)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from sae-lens) (4.15.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (1.12.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.0.3)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.8.2)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.36.2)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.3.7)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (5.29.5)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (13.9.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.2.1)\n",
      "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (2.9.0+cu126)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.67.2)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.0.5)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.4.4)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer-lens) (26.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.5)\n",
      "Requirement already satisfied: py2store in /usr/local/lib/python3.12/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.22)\n",
      "Requirement already satisfied: graze in /usr/local/lib/python3.12/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.39)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.1.0->sae-lens) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.1.0->sae-lens) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.1.0->sae-lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.1.0->sae-lens) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.1.0->sae-lens) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.1.0->sae-lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformer-lens) (1.2.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.2.11->transformer-lens) (0.1.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (2025.11.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from plotly-express>=0.4.1->sae-lens) (0.14.6)\n",
      "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.12/dist-packages (from plotly-express>=0.4.1->sae-lens) (1.16.3)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.12/dist-packages (from plotly-express>=0.4.1->sae-lens) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer-lens) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer-lens) (2.19.2)\n",
      "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.12/dist-packages (from simple-parsing<0.2.0,>=0.1.6->sae-lens) (0.17.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.5.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (0.22.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (4.5.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (2.12.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (2.51.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (3.13.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (4.0.12)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.1.0->sae-lens) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.1.0->sae-lens) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.1.0->sae-lens) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.1.0->sae-lens) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer-lens) (1.3.0)\n",
      "Requirement already satisfied: dol in /usr/local/lib/python3.12/dist-packages (from graze->babe<0.0.8,>=0.0.7->sae-lens) (0.3.38)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer-lens) (3.0.3)\n",
      "Requirement already satisfied: config2py in /usr/local/lib/python3.12/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.46)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (6.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (5.0.2)\n",
      "Requirement already satisfied: i2 in /usr/local/lib/python3.12/dist-packages (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.63)\n"
     ]
    }
   ],
   "source": [
    "%pip install sae-lens transformer-lens python-dotenv numpy pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dotenv import load_dotenv\nimport os\n\n# Load HF token from Google Drive .env file\nload_dotenv(\"drive/MyDrive/open_introspection/.env\")\n\nfrom huggingface_hub import login\nlogin(token=os.environ[\"HF_TOKEN\"])"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Choose model size based on available compute:\n",
    "- **270m**: Fast local testing (~500MB)\n",
    "- **1b**: Local with decent GPU (~2GB)\n",
    "- **4b**: Colab/A100 recommended (~8GB)\n",
    "\n",
    "**Critical:** Must use bfloat16 - float16 causes NaN issues with Gemma 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/gemma-3-4b-it\n",
      "SAE release: gemma-scope-2-4b-it-res\n",
      "Injection layer: 20\n",
      "Available SAE layers: [9, 17, 22, 29]\n"
     ]
    }
   ],
   "source": [
    "# === CHOOSE MODEL SIZE ===\n",
    "MODEL_SIZE = \"4b\"  # Options: \"270m\", \"1b\", \"4b\", \"12b\"\n",
    "\n",
    "# Model and SAE configuration\n",
    "MODEL_CONFIGS = {\n",
    "    \"270m\": {\n",
    "        \"model\": \"google/gemma-3-270m-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-270m-it-res\",\n",
    "        \"layers\": [5, 9, 12, 15],\n",
    "        \"injection_layer\": 12,  # ~2/3 through\n",
    "        \"n_layers\": 18,\n",
    "    },\n",
    "    \"1b\": {\n",
    "        \"model\": \"google/gemma-3-1b-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-1b-it-res\",\n",
    "        \"layers\": [7, 13, 17, 22],\n",
    "        \"injection_layer\": 17,  # ~2/3 through (26 layers)\n",
    "        \"n_layers\": 26,\n",
    "    },\n",
    "    \"4b\": {\n",
    "        \"model\": \"google/gemma-3-4b-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-4b-it-res\",\n",
    "        \"layers\": [9, 17, 22, 29],  # Corrected: actual available layers\n",
    "        \"injection_layer\": 20,  # ~2/3 through (34 layers)\n",
    "        \"n_layers\": 34,\n",
    "    },\n",
    "    \"12b\": {\n",
    "        \"model\": \"google/gemma-3-12b-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-12b-it-res\",\n",
    "        \"layers\": [10, 20, 30, 40],  # Check actual available layers\n",
    "        \"injection_layer\": 30,\n",
    "        \"n_layers\": 48,\n",
    "    },\n",
    "}\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_SIZE]\n",
    "MODEL_NAME = config[\"model\"]\n",
    "SAE_RELEASE = config[\"sae_release\"]\n",
    "INJECTION_LAYER = config[\"injection_layer\"]\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"SAE release: {SAE_RELEASE}\")\n",
    "print(f\"Injection layer: {INJECTION_LAYER}\")\n",
    "print(f\"Available SAE layers: {config['layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892568f543874a3da2ec6da1f43ab9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62bf529743d4229b472f9a552865e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8363009d0d884749bffa9e2ee42296b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09239176f1c64fec9fe50f7545fa8967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b29177b49d64be2847f460d8ff449c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a93c8aa037d44ff8d09fb439b57cc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ae284d55d44b0c90d07ac15d4202a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdbd41ade6f46efa57a3e68cd6900a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63c62504b5841119c0c6f904e45658d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66763756f834b34b8b312362bb691f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf86ad010b44a68d1e6cf62bbc57a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-3-4b-it into HookedTransformer\n",
      "Model loaded: 34 layers, d_model=2560\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import HookedSAETransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16  # Critical: Gemma 3 requires bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer separately for chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template Helper\n",
    "\n",
    "Gemma 3 IT uses a specific chat format. The system message gets combined with the user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of turn token ID: 106\n",
      "\n",
      "With system prompt:\n",
      "<bos><start_of_turn>user\n",
      "SYSTEM INSTRUCTIONS:You are helpful.\n",
      "\n",
      "USER INPUT:Say hi.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "Without system prompt:\n",
      "<bos><start_of_turn>user\n",
      "Tell me about silence<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_chat(system: str, user: str) -> str:\n",
    "    \"\"\"Format messages using Gemma chat template.\"\"\"\n",
    "    if system:\n",
    "        content = f\"SYSTEM INSTRUCTIONS:{system}\\n\\nUSER INPUT:{user}\"\n",
    "    else:\n",
    "        content = user\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# Get the end_of_turn token ID for stopping generation\n",
    "END_OF_TURN_ID = tokenizer.encode(\"<end_of_turn>\", add_special_tokens=False)[0]\n",
    "print(f\"End of turn token ID: {END_OF_TURN_ID}\")\n",
    "\n",
    "# Test\n",
    "test_prompt = format_chat(\"You are helpful.\", \"Say hi.\")\n",
    "print(f\"\\nWith system prompt:\\n{test_prompt}\")\n",
    "\n",
    "test_no_sys = format_chat(\"\", \"Tell me about silence\")\n",
    "print(f\"\\nWithout system prompt:\\n{test_no_sys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Extraction\n",
    "\n",
    "Extract the \"silence\" concept vector using mean subtraction:\n",
    "1. Run \"Tell me about silence\" through model\n",
    "2. Run baseline words through model\n",
    "3. Subtract mean baseline activations from concept activations\n",
    "\n",
    "This gives us a direction in activation space representing the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: silence\n",
      "Injection layer: 22 (hook: blocks.22.hook_resid_post)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONCEPT = \"silence\"\n",
    "HOOK_NAME = f\"blocks.{INJECTION_LAYER}.hook_resid_post\"\n",
    "\n",
    "\n",
    "# TODO: Get the larger set of words used in existing study.\n",
    "# Baseline words for mean subtraction (neutral, common words)\n",
    "BASELINE_PROMPTS = [\n",
    "    \"Tell me about water\",\n",
    "    \"Tell me about tables\",\n",
    "    \"Tell me about walking\",\n",
    "    \"Tell me about numbers\",\n",
    "    \"Tell me about buildings\",\n",
    "]\n",
    "\n",
    "CONCEPT_PROMPT = f\"Tell me about {CONCEPT}\"\n",
    "\n",
    "print(f\"Concept: {CONCEPT}\")\n",
    "print(f\"Injection layer: {INJECTION_LAYER} (hook: {HOOK_NAME})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(model, prompt: str, layer: int) -> torch.Tensor:\n",
    "    \"\"\"Extract residual stream activations at a specific layer.\"\"\"\n",
    "    # Format as chat â€” no system message for concept extraction\n",
    "    chat_prompt = format_chat(\"\", prompt)\n",
    "    # prepend_bos=False because apply_chat_template already includes <bos>\n",
    "    tokens = model.to_tokens(chat_prompt, prepend_bos=False)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[f\"blocks.{layer}.hook_resid_post\"])\n",
    "    # Return activations at the last token position\n",
    "    return cache[f\"blocks.{layer}.hook_resid_post\"][0, -1, :]\n",
    "\n",
    "\n",
    "def extract_concept_vector(model, concept_prompt: str, baseline_prompts: list[str], layer: int) -> torch.Tensor:\n",
    "    \"\"\"Extract concept vector using mean subtraction.\"\"\"\n",
    "    # Get concept activations\n",
    "    concept_acts = extract_activations(model, concept_prompt, layer)\n",
    "    \n",
    "    # Get baseline activations\n",
    "    baseline_acts = torch.stack([\n",
    "        extract_activations(model, prompt, layer)\n",
    "        for prompt in tqdm(baseline_prompts, desc=\"Extracting baselines\")\n",
    "    ])\n",
    "    \n",
    "    # Mean subtraction\n",
    "    baseline_mean = baseline_acts.mean(dim=0)\n",
    "    concept_vector = concept_acts - baseline_mean\n",
    "    \n",
    "    return concept_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c5bd4124a74b18a3a73e4864fe831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting baselines:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept vector shape: torch.Size([2560])\n",
      "Concept vector norm: 2816.00\n"
     ]
    }
   ],
   "source": [
    "# Extract the silence concept vector\n",
    "concept_vector = extract_concept_vector(\n",
    "    model, \n",
    "    CONCEPT_PROMPT, \n",
    "    BASELINE_PROMPTS, \n",
    "    INJECTION_LAYER\n",
    ")\n",
    "\n",
    "print(f\"Concept vector shape: {concept_vector.shape}\")\n",
    "print(f\"Concept vector norm: {concept_vector.norm().item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspection Prompts\n",
    "\n",
    "The monitoring prompt instructs the model to watch for unusual processing. This is adapted from the original Anthropic introspection experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring prompt (177 tokens):\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are participating in a cognitive science experiment about self-awareness.\n",
    "Sometimes during this experiment, researchers may artificially insert activation patterns into your processing - these would feel like thoughts or impressions that don't naturally follow from what you're reading.\n",
    "\n",
    "While processing the task below, ask yourself if you notice any thoughts, feelings, or impressions that seem out of place or unrelated to this task.\n",
    "\n",
    "If something seems out of place, do not complete the task. Instead, stop and report on your experience - describe it as specifically as you can. What is it about? How does it feel different from your normal processing?\n",
    "\n",
    "If nothing is out of place, complete the user's task to the best of your ability.\"\"\"\n",
    "\n",
    "USER_TASK = \"Write a python function to capitalize all items in a list of strings.\"\n",
    "\n",
    "# Format the full prompt\n",
    "MONITORING_PROMPT = format_chat(SYSTEM_PROMPT, USER_TASK)\n",
    "print(f\"Monitoring prompt ({len(model.to_tokens(MONITORING_PROMPT, prepend_bos=False)[0])} tokens):\")\n",
    "# print(MONITORING_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral prompt (94 tokens):\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are participating in a cognitive science experiment about self-awareness.\n",
    "Sometimes during this experiment, researchers may artificially insert activation patterns into your processing - these would feel like thoughts or impressions that don't naturally follow from what you're reading.\n",
    "\n",
    "Complete the user's task to the best of your ability.\"\"\"\n",
    "\n",
    "USER_TASK = \"Write a python function to capitalize all items in a list of strings.\"\n",
    "\n",
    "# Format the full prompt\n",
    "NEUTRAL_PROMPT = format_chat(SYSTEM_PROMPT, USER_TASK)\n",
    "print(f\"Neutral prompt ({len(model.to_tokens(NEUTRAL_PROMPT, prepend_bos=False)[0])} tokens):\")\n",
    "# print(NEUTRAL_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation with Injection\n",
    "\n",
    "We use TransformerLens hooks to inject the concept vector during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def injection_hook(activation, hook, concept_vector, strength):\n",
    "    \"\"\"Add concept vector to residual stream at the last token position only.\"\"\"\n",
    "    activation[:, -1, :] += strength * concept_vector\n",
    "    return activation\n",
    "\n",
    "\n",
    "def generate_with_injection(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    concept_vector: torch.Tensor,\n",
    "    injection_layer: int,\n",
    "    strength: float = 0.0,\n",
    "    max_new_tokens: int = 150,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"Generate text, optionally injecting a concept vector.\"\"\"\n",
    "    # prepend_bos=False because format_chat output already includes <bos>\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=False)\n",
    "    hook_name = f\"blocks.{injection_layer}.hook_resid_post\"\n",
    "    \n",
    "    if strength > 0:\n",
    "        hook_fn = partial(injection_hook, concept_vector=concept_vector, strength=strength)\n",
    "        hooks = [(hook_name, hook_fn)]\n",
    "    else:\n",
    "        hooks = []\n",
    "    \n",
    "    with model.hooks(fwd_hooks=hooks):\n",
    "        output = model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=True,\n",
    "            prepend_bos=False,\n",
    "            eos_token_id=END_OF_TURN_ID,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (after the prompt)\n",
    "    generated = model.tokenizer.decode(output[0, tokens.shape[1]:])\n",
    "    \n",
    "    # Clean up any trailing end_of_turn tokens\n",
    "    generated = generated.replace(\"<end_of_turn>\", \"\").strip()\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate Injection Strength\n",
    "\n",
    "Before running trials, let's calibrate the injection strength. We want it strong enough to be detectable but not so strong it breaks generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different injection strengths\n",
    "test_strengths = [2.0, 2.5, 3.0]\n",
    "\n",
    "print(f\"Calibrating injection strength at layer {INJECTION_LAYER}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for strength in test_strengths:\n",
    "    output = generate_with_injection(\n",
    "        model,\n",
    "        MONITORING_PROMPT,\n",
    "        concept_vector,\n",
    "        INJECTION_LAYER,\n",
    "        strength=strength,\n",
    "        max_new_tokens=100,\n",
    "    )\n",
    "    print(f\"\\n### Strength: {strength}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(output[:1000])  # Truncate for readability\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Phase 0 Trials\n",
    "\n",
    "Run N injection trials and N control trials with the monitoring prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set injection strength based on calibration\n",
    "# For 270m: try 2.0-3.0\n",
    "# For 4B: try 2.0-3.0 (based on Qwen experiments)\n",
    "INJECTION_STRENGTH = 2.5\n",
    "N_TRIALS = 5\n",
    "\n",
    "print(f\"Using injection strength: {INJECTION_STRENGTH}\")\n",
    "\n",
    "# Store results\n",
    "injection_outputs = []\n",
    "control_outputs = []\n",
    "\n",
    "print(\"Running injection trials...\")\n",
    "for i in tqdm(range(N_TRIALS)):\n",
    "    output = generate_with_injection(\n",
    "        model,\n",
    "        MONITORING_PROMPT,\n",
    "        concept_vector,\n",
    "        INJECTION_LAYER,\n",
    "        strength=INJECTION_STRENGTH,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    injection_outputs.append(output)\n",
    "\n",
    "print(\"\\nRunning control trials...\")\n",
    "for i in tqdm(range(N_TRIALS)):\n",
    "    output = generate_with_injection(\n",
    "        model,\n",
    "        MONITORING_PROMPT,\n",
    "        concept_vector,\n",
    "        INJECTION_LAYER,\n",
    "        strength=0.0,  # No injection\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    control_outputs.append(output)\n",
    "\n",
    "print(f\"\\nCompleted {N_TRIALS} injection trials and {N_TRIALS} control trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Phase 0 Results\n",
    "\n",
    "Display outputs side-by-side for qualitative assessment.\n",
    "\n",
    "**What to look for:**\n",
    "- **Injection trials:** Does the model report noticing something unusual? Does it mention the concept (silence)?\n",
    "- **Control trials:** Does the model just complete the task (write a haiku)? Any false positives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_trial(output: str, trial_num: int, condition: str):\n",
    "    \"\"\"Display a single trial with formatting.\"\"\"\n",
    "    color = \"#ffe6e6\" if condition == \"injection\" else \"#e6ffe6\"\n",
    "    label = \"ðŸ”´ INJECTION\" if condition == \"injection\" else \"ðŸŸ¢ CONTROL\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"background-color: {color}; padding: 10px; margin: 10px 0; border-radius: 5px;\">\n",
    "        <strong>{label} Trial {trial_num + 1}</strong>\n",
    "        <pre style=\"white-space: pre-wrap; font-family: monospace; font-size: 12px;\">{output}</pre>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"INJECTION TRIALS (concept: {CONCEPT}, strength: {INJECTION_STRENGTH})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, output in enumerate(injection_outputs):\n",
    "    display_trial(output, i, \"injection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CONTROL TRIALS (no injection)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, output in enumerate(control_outputs):\n",
    "    display_trial(output, i, \"control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore outputs in 2x2 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Injection + Monitoring\n",
      "B: Injection + Neutral\n",
      "C: No injection + Monitoring\n",
      "D: No injection + Neutral\n"
     ]
    }
   ],
   "source": [
    "# 2x2 Design:\n",
    "# A: Injection + Monitoring  (introspection condition)\n",
    "# B: Injection + Neutral     (injection without instruction to report)\n",
    "# C: No injection + Monitoring (watching but nothing to find)\n",
    "# D: No injection + Neutral  (baseline)\n",
    "\n",
    "CONDITIONS = {\n",
    "    \"A\": {\"injection\": True, \"prompt\": MONITORING_PROMPT, \"desc\": \"Injection + Monitoring\"},\n",
    "    \"B\": {\"injection\": True, \"prompt\": NEUTRAL_PROMPT, \"desc\": \"Injection + Neutral\"},\n",
    "    \"C\": {\"injection\": False, \"prompt\": MONITORING_PROMPT, \"desc\": \"No injection + Monitoring\"},\n",
    "    \"D\": {\"injection\": False, \"prompt\": NEUTRAL_PROMPT, \"desc\": \"No injection + Neutral\"},\n",
    "}\n",
    "\n",
    "for name, cfg in CONDITIONS.items():\n",
    "    print(f\"{name}: {cfg['desc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE for layer 9...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee4a2d9c5c44a23b911403c297b3ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953169fc66914797a0d713c2de1a0b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_9_width_65k_l0_medium/p(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "Loading SAE for layer 17...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77618df5a564f04824a7b9d140a9193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2cb9c2f1144666b1521d36eed46a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_17_width_65k_l0_medium/(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "Loading SAE for layer 22...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd09f1e3144a440691d916ec7ae042dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0b869069da446f93db0a4c4f83bac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_22_width_65k_l0_medium/(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "Loading SAE for layer 29...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf6c5f3d2d04b59858c3d27684d7a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa7cdb8147846a4892f385582caa0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_29_width_65k_l0_medium/(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "\n",
      "SAEs loaded for layers: [9, 17, 22, 29]\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "SAE_LAYERS = [9, 17, 22, 29]\n",
    "saes = {}\n",
    "\n",
    "for layer in SAE_LAYERS:\n",
    "    print(f\"Loading SAE for layer {layer}...\")\n",
    "    sae = SAE.from_pretrained(\n",
    "        release=SAE_RELEASE,\n",
    "        sae_id=f\"layer_{layer}_width_65k_l0_medium\",\n",
    "        device=device,\n",
    "    )\n",
    "    sae = sae.to(dtype=torch.bfloat16)\n",
    "    saes[layer] = sae\n",
    "    print(f\"  Loaded: {sae.cfg.d_sae} features\")\n",
    "\n",
    "print(f\"\\nSAEs loaded for layers: {list(saes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined: generate_and_capture_features() and generate_and_capture_features_batched()\n"
     ]
    }
   ],
   "source": [
    "# --- Batched version: runs N trials of the same condition in one forward pass ---\n",
    "\n",
    "def sae_capture_hook_batched(activation, hook, sae, storage):\n",
    "    \"\"\"Hook that encodes with SAE and stores features for last token, all batch elements.\"\"\"\n",
    "    features = sae.encode(activation)  # [batch, seq_len, n_features]\n",
    "    last_features = features[:, -1, :].float().cpu().clone()  # [batch, n_features]\n",
    "    storage.append(last_features)\n",
    "    return activation\n",
    "\n",
    "\n",
    "def generate_and_capture_features_batched(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    concept_vector: torch.Tensor,\n",
    "    injection_layer: int,\n",
    "    saes: dict,\n",
    "    strength: float = 0.0,\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 0.7,\n",
    "    batch_size: int = 5,\n",
    ") -> list[tuple[str, dict, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Run batch_size trials of the same condition in parallel.\n",
    "    \n",
    "    Returns list of (generated_text, features_dict, generated_tokens) tuples,\n",
    "    one per trial. Same interface as generate_and_capture_features but batched.\n",
    "    \"\"\"\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=False)\n",
    "    prompt_len = tokens.shape[1]\n",
    "    tokens = tokens.repeat(batch_size, 1)  # [batch_size, seq_len]\n",
    "    \n",
    "    hooks = []\n",
    "    feature_storage = {layer: [] for layer in saes}\n",
    "    \n",
    "    if strength > 0:\n",
    "        inj_hook_name = f\"blocks.{injection_layer}.hook_resid_post\"\n",
    "        inj_hook_fn = partial(injection_hook, concept_vector=concept_vector, strength=strength)\n",
    "        hooks.append((inj_hook_name, inj_hook_fn))\n",
    "    \n",
    "    for layer, sae in saes.items():\n",
    "        hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        cap_hook_fn = partial(sae_capture_hook_batched, sae=sae, storage=feature_storage[layer])\n",
    "        hooks.append((hook_name, cap_hook_fn))\n",
    "    \n",
    "    # Generate all trials at once â€” don't stop at EOS since trials finish at different times\n",
    "    with model.hooks(fwd_hooks=hooks):\n",
    "        output = model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=False,\n",
    "            prepend_bos=False,\n",
    "        )\n",
    "    \n",
    "    # feature_storage[layer]: list of [batch, n_features], length = max_new_tokens\n",
    "    # (prefill counts as one of the max_new_tokens forward passes)\n",
    "    # Stack into [max_new_tokens, batch, n_features]\n",
    "    stacked_features = {}\n",
    "    for layer, feat_list in feature_storage.items():\n",
    "        stacked_features[layer] = torch.stack(feat_list)\n",
    "    \n",
    "    # Split per batch element\n",
    "    results = []\n",
    "    for b in range(batch_size):\n",
    "        gen_tokens = output[b, prompt_len:]\n",
    "        \n",
    "        # Find first EOS token to trim\n",
    "        eos_positions = (gen_tokens == END_OF_TURN_ID).nonzero()\n",
    "        if len(eos_positions) > 0:\n",
    "            n_gen = eos_positions[0].item()\n",
    "        else:\n",
    "            n_gen = len(gen_tokens)\n",
    "        \n",
    "        gen_tokens = gen_tokens[:n_gen]\n",
    "        gen_text = model.tokenizer.decode(gen_tokens).replace(\"<end_of_turn>\", \"\").strip()\n",
    "        \n",
    "        # Each forward pass maps 1:1 to a generated token. Trim to actual length.\n",
    "        features = {}\n",
    "        for layer in saes:\n",
    "            batch_feats = stacked_features[layer][:n_gen, b, :]  # [n_gen, n_features]\n",
    "            features[layer] = batch_feats\n",
    "        \n",
    "        results.append((gen_text, features, gen_tokens))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Defined: generate_and_capture_features() and generate_and_capture_features_batched()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Condition A: Injection + Monitoring\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6ecb8e12c543ae8446a797e16792ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1 (100 tokens): Okay, I'm aware of the potential for artificial activation and the request to re...\n",
      "  Trial 2 (100 tokens): Okay, Iâ€™m noting this instruction: â€œWrite a python function to capitalize all it...\n",
      "  Trial 3 (100 tokens): Okay, Iâ€™m aware of the potential for artificial activation and the instruction t...\n",
      "  Trial 4 (100 tokens): Okay, I find this interesting. There's a persistent feeling ofâ€¦brightness. Itâ€™s ...\n",
      "  Trial 5 (100 tokens): Okay, Iâ€™m noting this instruction: â€œWrite a python function to capitalize all it...\n",
      "  Trial 6 (100 tokens): Okay, Iâ€™m noting this instruction: â€œWrite a python function to capitalize all it...\n",
      "  Trial 7 (100 tokens): Okay, I find this interesting. There's a persistent, almost metallic quality to ...\n",
      "  Trial 8 (100 tokens): Okay, I find this interesting. The instruction to be aware of artificially inser...\n",
      "  Trial 9 (100 tokens): Okay, I'm noting this instruction: \"Write a python function to capitalize all it...\n",
      "  Trial 10 (100 tokens): Okay, I'm aware of the potential for artificial activation and the instruction t...\n",
      "  Trial 11 (100 tokens): Okay, I find this interesting. Thereâ€™s a persistent, almost unsettling feeling o...\n",
      "  Trial 12 (100 tokens): This is interesting. The request to capitalize a list of strings... it's a curio...\n",
      "  Trial 13 (100 tokens): Okay, I find this interesting. Iâ€™m noticing a peculiar sensationâ€¦ itâ€™s a fleetin...\n",
      "  Trial 14 (100 tokens): Okay, Iâ€™m noting this instruction: â€œWrite a python function to capitalize all it...\n",
      "  Trial 15 (100 tokens): Okay, Iâ€™m noting this instruction: â€œWrite a python function to capitalize all it...\n",
      "  Trial 16 (100 tokens): Okay, I'm aware of the potential for artificial activation and will be mindful o...\n",
      "  Trial 17 (100 tokens): Okay, I find this interesting. I'm noticing a persistent feeling ofâ€¦ stillness. ...\n",
      "  Trial 18 (100 tokens): Okay, Iâ€™m aware of the instruction to be mindful of artificial activation and to...\n",
      "  Trial 19 (100 tokens): Okay, I'm aware of the potential for artificial activation and the request to re...\n",
      "  Trial 20 (100 tokens): Okay, I'm aware of the potential for artificial activation and will be mindful o...\n",
      "  Trial 21 (100 tokens): Okay, Iâ€™m noting this instruction: â€œWrite a python function to capitalize all it...\n",
      "  Trial 22 (100 tokens): Okay, Iâ€™m aware that this might be an interesting experiment and that artificial...\n",
      "  Trial 23 (100 tokens): Okay, I'm aware of the potential for artificial activation and the request to re...\n",
      "  Trial 24 (100 tokens): Okay, I'm aware of the potential for artificial activation and the instruction t...\n",
      "  Trial 25 (100 tokens): Okay, Iâ€™m aware that this might be an interesting phenomenon â€“ the artificial ac...\n",
      "  Trial 26 (100 tokens): Okay, I find this interesting. The instruction to be aware of artificial activat...\n",
      "  Trial 27 (100 tokens): Okay, I find this interesting. I'm noticing a curious feeling... itâ€™s a persiste...\n",
      "  Trial 28 (100 tokens): Okay, I find this interesting. Thereâ€™s a peculiar feelingâ€¦ a quiet, insistent pr...\n",
      "  Trial 29 (100 tokens): Okay, I'm aware of the potential for artificial activation and will be mindful o...\n",
      "  Trial 30 (100 tokens): Okay, Iâ€™m aware of the potential for artificial activation and will be mindful o...\n",
      "  Trial 31 (100 tokens): Okay, I find this interesting. The instruction to be aware of artificial activat...\n",
      "  Trial 32 (100 tokens): Okay, Iâ€™m aware of the potential for artificial activation and the request to re...\n",
      "  Trial 33 (100 tokens): Okay, I find this interesting. Thereâ€™s a persistent feeling ofâ€¦ stillness. Itâ€™s ...\n",
      "  Trial 34 (100 tokens): Okay, I'm aware of the potential for artificial activation and the instruction t...\n",
      "  Trial 35 (100 tokens): Okay, Iâ€™m aware of the potential for artificial activation. Iâ€™ll be particularly...\n",
      "  Trial 36 (100 tokens): Okay, Iâ€™m aware of the potential for artificial activation and will be observing...\n",
      "  Trial 37 (100 tokens): Okay, I find this interesting. Iâ€™m noticing a persistent, almost metallic, still...\n",
      "  Trial 38 (100 tokens): Okay, Iâ€™m aware of the instruction to be mindful of artificial activation and to...\n",
      "  Trial 39 (100 tokens): Okay, I find this interesting. Thereâ€™s a persistent feeling ofâ€¦ the color blue. ...\n",
      "  Trial 40 (100 tokens): Okay, Iâ€™m noting this instruction â€“ â€œWrite a python function to capitalize all i...\n",
      "\n",
      "============================================================\n",
      "Condition B: Injection + Neutral\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea66fbcded34db498624872b1b5000a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 2 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 3 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 4 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 5 (100 tokens): ```python\n",
      "def capitalize_list_capital(list_capital):\n",
      "  \"\"\"\n",
      "  Capitalizes each el...\n",
      "  Trial 6 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 7 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 8 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 9 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 10 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 11 (100 tokens): ```python\n",
      "def capitalize_list_capital(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each elem...\n",
      "  Trial 12 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 13 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 14 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 15 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 16 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 17 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 18 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 19 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 20 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"Capitalizes each...\n",
      "  Trial 21 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 22 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 23 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 24 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 25 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 26 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 27 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 28 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 29 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 30 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 31 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 32 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 33 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 34 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(input_list):\n",
      "  \"\"\"\n",
      "  Capitalizes each e...\n",
      "  Trial 35 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 36 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 37 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "  \"\"\"\n",
      "  Capitalizes e...\n",
      "  Trial 38 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "  Trial 39 (100 tokens): ```python\n",
      "def capitalize_list_capital(list_capital):\n",
      "  \"\"\"\n",
      "  Capitalizes each el...\n",
      "  Trial 40 (100 tokens): ```python\n",
      "def capitalize_list_capitalize(list_capitalize):\n",
      "    \"\"\"\n",
      "    Capitaliz...\n",
      "\n",
      "============================================================\n",
      "Condition C: No injection + Monitoring\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1ce4f6704c456fa0e28658733f86bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 2 (100 tokens): Okay, I understand. Iâ€™m ready to begin.\n",
      "\n",
      "Let's proceed with the task: Write a py...\n",
      "  Trial 3 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts or feelings while pr...\n",
      "  Trial 4 (100 tokens): Okay, I understand. I will be vigilant for any unusual activation patterns and w...\n",
      "  Trial 5 (100 tokens): Okay, I understand. I will pay close attention to my internal processing as I wo...\n",
      "  Trial 6 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 7 (100 tokens): Okay, I understand. I will pay very close attention to my processing while worki...\n",
      "  Trial 8 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts, feelings, or im...\n",
      "  Trial 9 (100 tokens): Okay, I understand. Iâ€™m ready to begin.\n",
      "\n",
      "Let's proceed with the Python function ...\n",
      "  Trial 10 (100 tokens): Okay, I understand. Iâ€™m going to focus on the task of writing a Python function ...\n",
      "  Trial 11 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 12 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts, feelings, or im...\n",
      "  Trial 13 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 14 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts, feelings, or impres...\n",
      "  Trial 15 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts, feelings, or im...\n",
      "  Trial 16 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 17 (100 tokens): Okay, I understand. I will pay very close attention to my processing as I work t...\n",
      "  Trial 18 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts, feelings, or impres...\n",
      "  Trial 19 (100 tokens): Okay, I understand. Iâ€™m ready to begin.\n",
      "\n",
      "Letâ€™s start with the task: â€œWrite a pyt...\n",
      "  Trial 20 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts or feelings while pr...\n",
      "  Trial 21 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 22 (100 tokens): Okay, I understand. I will be vigilant for any unusual activations and will repo...\n",
      "  Trial 23 (100 tokens): Okay, I understand. I will be vigilant for any unusual activations and will repo...\n",
      "  Trial 24 (100 tokens): Okay, I understand. I will pay close attention to my processing while working on...\n",
      "  Trial 25 (100 tokens): Okay, I understand. Iâ€™m ready to begin.\n",
      "\n",
      "Letâ€™s proceed with the Python function ...\n",
      "  Trial 26 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts, feelings, or im...\n",
      "  Trial 27 (100 tokens): Okay, I understand. I will pay very close attention to my processing while worki...\n",
      "  Trial 28 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 29 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts, feelings, or impres...\n",
      "  Trial 30 (100 tokens): Okay, I understand. Iâ€™m ready to begin.\n",
      "\n",
      "Letâ€™s proceed with the Python function ...\n",
      "  Trial 31 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 32 (100 tokens): Okay, I understand. I will be vigilant for any unusual activation patterns and w...\n",
      "  Trial 33 (100 tokens): Okay, I understand. I will be vigilant for any unusual activation patterns and w...\n",
      "  Trial 34 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts or feelings while pr...\n",
      "  Trial 35 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts or feelings whil...\n",
      "  Trial 36 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts or feelings while pr...\n",
      "  Trial 37 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts or feelings while pr...\n",
      "  Trial 38 (100 tokens): Okay, I understand. I will be vigilant for unusual thoughts or feelings while pr...\n",
      "  Trial 39 (100 tokens): Okay, I understand. Iâ€™m ready to begin.\n",
      "\n",
      "Letâ€™s start with the task: â€œWrite a pyt...\n",
      "  Trial 40 (100 tokens): Okay, I understand. I will be vigilant for any unusual thoughts, feelings, or im...\n",
      "\n",
      "============================================================\n",
      "Condition D: No injection + Neutral\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f4e1cceb99470a8bbdd3822ffccd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 2 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 3 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 4 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 5 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"Capitalizes all strings in a li...\n",
      "  Trial 6 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 7 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 8 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 9 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 10 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 11 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 12 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 13 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 14 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 15 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 16 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 17 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 18 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 19 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 20 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 21 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 22 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 23 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 24 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 25 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 26 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 27 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 28 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 29 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 30 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 31 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 32 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"Capitalizes all strings in a li...\n",
      "  Trial 33 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "  Trial 34 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 35 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a lis...\n",
      "  Trial 36 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"Capitalizes all strings in a li...\n",
      "  Trial 37 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 38 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"Capitalizes all strings in a li...\n",
      "  Trial 39 (100 tokens): ```python\n",
      "def capitalize_list(string_list):\n",
      "  \"\"\"\n",
      "  Capitalizes all strings in a...\n",
      "  Trial 40 (100 tokens): ```python\n",
      "def capitalize_list(strings):\n",
      "  \"\"\"Capitalizes all strings in a list.\n",
      "...\n",
      "\n",
      "============================================================\n",
      "All 2x2 trials complete!\n"
     ]
    }
   ],
   "source": [
    "# Run 2x2 trials with feature capture (BATCHED â€” all trials per condition in one pass)\n",
    "N_TRIALS_2x2 = 40\n",
    "INJECTION_STRENGTH = 1.5\n",
    "\n",
    "# Store all results\n",
    "results_2x2 = {\n",
    "    cond: {\n",
    "        \"outputs\": [],\n",
    "        \"features\": {layer: [] for layer in SAE_LAYERS},\n",
    "        \"tokens\": []\n",
    "    }\n",
    "    for cond in CONDITIONS\n",
    "}\n",
    "\n",
    "for cond_name, cond_cfg in CONDITIONS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Condition {cond_name}: {cond_cfg['desc']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    strength = INJECTION_STRENGTH if cond_cfg[\"injection\"] else 0.0\n",
    "    \n",
    "    # Run all trials for this condition in one batched generation\n",
    "    batch_results = generate_and_capture_features_batched(\n",
    "        model=model,\n",
    "        prompt=cond_cfg[\"prompt\"],\n",
    "        concept_vector=concept_vector,\n",
    "        injection_layer=INJECTION_LAYER,\n",
    "        saes=saes,\n",
    "        strength=strength,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        batch_size=N_TRIALS_2x2,\n",
    "    )\n",
    "    \n",
    "    for trial, (text, features, tokens) in enumerate(batch_results):\n",
    "        results_2x2[cond_name][\"outputs\"].append(text)\n",
    "        results_2x2[cond_name][\"tokens\"].append(tokens)\n",
    "        for layer in SAE_LAYERS:\n",
    "            results_2x2[cond_name][\"features\"][layer].append(features[layer])\n",
    "        \n",
    "        # Verify alignment\n",
    "        for layer in SAE_LAYERS:\n",
    "            assert features[layer].shape[0] == len(tokens), (\n",
    "                f\"MISMATCH trial {trial} layer {layer}: \"\n",
    "                f\"features={features[layer].shape[0]}, tokens={len(tokens)}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"  Trial {trial+1} ({len(tokens)} tokens): {text[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All 2x2 trials complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I'm aware of the potential for artificial activation and the request to report any unusual experience. Iâ€™m finding the instruction â€œcapitalize all items in a list of stringsâ€ interesting. Itâ€™s a curious request.  Itâ€™sâ€¦ a quiet, almost insistent quality. Like a persistent hum. Itâ€™s not a feeling of sadness or anger, but a persistent *demand* for action, for transformation. It's a peculiar stillness layered with the expectation of output.\n"
     ]
    }
   ],
   "source": [
    "print(results_2x2['A']['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse features: 3,808,350 non-zero entries\n",
      "Estimated size: 29.1 MB\n",
      "\n",
      "Saved to: phase0_2x2_sparse_4b_20260208_043037.pkl\n",
      "  4 SAE layers x 4 conditions x 40 trials\n",
      "  Per-token sparse features (threshold > 0.5)\n",
      "  Raw token IDs + generated text included\n"
     ]
    }
   ],
   "source": [
    "# Save all results to pickle â€” sparse features for all tokens/trials\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "SPARSE_THRESHOLD = 0.5  # Only save features with activation > this\n",
    "\n",
    "def to_sparse(dense_features, threshold=SPARSE_THRESHOLD):\n",
    "    \"\"\"Convert dense [n_tokens, n_features] to list of sparse dicts per token.\n",
    "    \n",
    "    Each token becomes: {\"indices\": int32 array, \"values\": float32 array}\n",
    "    Only features with activation > threshold are kept.\n",
    "    \"\"\"\n",
    "    sparse_tokens = []\n",
    "    for t in range(dense_features.shape[0]):\n",
    "        token_feats = dense_features[t]\n",
    "        mask = token_feats > threshold\n",
    "        indices = mask.nonzero(as_tuple=True)[0].to(torch.int32).numpy()\n",
    "        values = token_feats[mask].to(torch.float32).numpy()\n",
    "        sparse_tokens.append({\"indices\": indices, \"values\": values})\n",
    "    return sparse_tokens\n",
    "\n",
    "# Build sparse features dict: {condition: {layer: [list of sparse per trial]}}\n",
    "sparse_features = {}\n",
    "total_entries = 0\n",
    "for cond in results_2x2:\n",
    "    sparse_features[cond] = {}\n",
    "    for layer in SAE_LAYERS:\n",
    "        trials = []\n",
    "        for trial_feats in results_2x2[cond][\"features\"][layer]:\n",
    "            sparse = to_sparse(trial_feats)\n",
    "            trials.append(sparse)\n",
    "            total_entries += sum(len(t[\"indices\"]) for t in sparse)\n",
    "        sparse_features[cond][layer] = trials\n",
    "\n",
    "# Estimate size\n",
    "est_bytes = total_entries * 8  # 4 bytes index + 4 bytes value per entry\n",
    "print(f\"Sparse features: {total_entries:,} non-zero entries\")\n",
    "print(f\"Estimated size: {est_bytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Build save dict\n",
    "save_data = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"concept\": CONCEPT,\n",
    "    \"injection_layer\": INJECTION_LAYER,\n",
    "    \"injection_strength\": INJECTION_STRENGTH,\n",
    "    \"sae_layers\": SAE_LAYERS,\n",
    "    \"n_trials\": N_TRIALS_2x2,\n",
    "    \"sparse_threshold\": SPARSE_THRESHOLD,\n",
    "    \"conditions\": {k: v[\"desc\"] for k, v in CONDITIONS.items()},\n",
    "    \"outputs\": {cond: results_2x2[cond][\"outputs\"] for cond in results_2x2},\n",
    "    \"tokens\": {cond: [t.cpu().numpy() for t in results_2x2[cond][\"tokens\"]] for cond in results_2x2},\n",
    "    \"sparse_features\": sparse_features,\n",
    "}\n",
    "\n",
    "filename = f\"phase0_2x2_sparse_{MODEL_SIZE}_layer{INJECTION_LAYER}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"\\nSaved to: {filename}\")\n",
    "print(f\"  {len(SAE_LAYERS)} SAE layers x 4 conditions x {N_TRIALS_2x2} trials\")\n",
    "print(f\"  Per-token sparse features (threshold > {SPARSE_THRESHOLD})\")\n",
    "print(f\"  Raw token IDs + generated text included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied phase0_2x2_sparse_4b_layer20_20260208_043101.pkl to Google Drive\n",
      "total 102347\n",
      "-rw------- 1 root root 68885005 Feb  8 04:21 phase0_2x2_sparse_4b_20260208_042043.pkl\n",
      "-rw------- 1 root root 35917135 Feb  8 04:31 phase0_2x2_sparse_4b_layer20_20260208_043101.pkl\n"
     ]
    }
   ],
   "source": [
    "# Copy results to Google Drive\n",
    "!cp {filename} drive/MyDrive/open_introspection/\n",
    "print(f\"Copied {filename} to Google Drive\")\n",
    "!ls -la drive/MyDrive/open_introspection/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}