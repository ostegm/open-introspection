{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0: Qualitative Verification of Introspection in Gemma 3 IT\n",
    "\n",
    "Before building full SAE feature analysis infrastructure, we need to verify that Gemma 3 shows introspection signal similar to what we observed in Qwen models.\n",
    "\n",
    "**Goal:** Run injection + control trials and manually assess whether the model can detect injected concepts.\n",
    "\n",
    "**Go/no-go criteria:**\n",
    "- If clear signal (model reports detecting something, names concept): proceed to Phase 1\n",
    "- If weak/no signal: try larger model before abandoning\n",
    "- If high false positives: adjust prompt or injection strength\n",
    "\n",
    "**Model options:**\n",
    "- Local testing: `google/gemma-3-270m-it` (layers 5, 9, 12, 15)\n",
    "- Colab/A100: `google/gemma-3-4b-it` (layers 7, 13, 17, 22, 27, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb  9 23:11:55 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sae-lens transformer-lens python-dotenv numpy pandas --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load HF token from Google Drive .env file\n",
    "load_dotenv(\"/content/drive/MyDrive/open_introspection/.env\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Choose model size based on available compute:\n",
    "- **270m**: Fast local testing (~500MB)\n",
    "- **1b**: Local with decent GPU (~2GB)\n",
    "- **4b**: Colab/A100 recommended (~8GB)\n",
    "\n",
    "**Critical:** Must use bfloat16 - float16 causes NaN issues with Gemma 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/gemma-3-4b-it\n",
      "SAE release: gemma-scope-2-4b-it-res\n",
      "Injection layer: 20\n",
      "Available SAE layers: [9, 17, 22, 29]\n"
     ]
    }
   ],
   "source": [
    "# === CHOOSE MODEL SIZE ===\n",
    "MODEL_SIZE = \"4b\"  # Options: \"270m\", \"1b\", \"4b\", \"12b\"\n",
    "\n",
    "# Model and SAE configuration\n",
    "MODEL_CONFIGS = {\n",
    "    \"270m\": {\n",
    "        \"model\": \"google/gemma-3-270m-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-270m-it-res\",\n",
    "        \"layers\": [5, 9, 12, 15],\n",
    "        \"injection_layer\": 12,  # ~2/3 through\n",
    "        \"n_layers\": 18,\n",
    "    },\n",
    "    \"1b\": {\n",
    "        \"model\": \"google/gemma-3-1b-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-1b-it-res\",\n",
    "        \"layers\": [7, 13, 17, 22],\n",
    "        \"injection_layer\": 17,  # ~2/3 through (26 layers)\n",
    "        \"n_layers\": 26,\n",
    "    },\n",
    "    \"4b\": {\n",
    "        \"model\": \"google/gemma-3-4b-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-4b-it-res\",\n",
    "        \"layers\": [9, 17, 22, 29],  # Corrected: actual available layers\n",
    "        \"injection_layer\": 20,  # ~2/3 through (34 layers)\n",
    "        \"n_layers\": 34,\n",
    "    },\n",
    "    \"12b\": {\n",
    "        \"model\": \"google/gemma-3-12b-it\",\n",
    "        \"sae_release\": \"gemma-scope-2-12b-it-res\",\n",
    "        \"layers\": [10, 20, 30, 40],  # Check actual available layers\n",
    "        \"injection_layer\": 30,\n",
    "        \"n_layers\": 48,\n",
    "    },\n",
    "}\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_SIZE]\n",
    "MODEL_NAME = config[\"model\"]\n",
    "SAE_RELEASE = config[\"sae_release\"]\n",
    "INJECTION_LAYER = config[\"injection_layer\"]\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"SAE release: {SAE_RELEASE}\")\n",
    "print(f\"Injection layer: {INJECTION_LAYER}\")\n",
    "print(f\"Available SAE layers: {config['layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b30619195841909191d34f4ac17a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94fd71a4ccc49c8a02cb08d7718d99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306358a986fe472480bafe71b6304c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aecd50965d4fb3afa2aee7dfbf2e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31af88bec759426eb337789791c44f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bb3293331b449db5d16f55eeec62ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e7d4a195c44502a7923ae9c2e6f56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2e90ad00b7482f8fe56af3c6d347b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dafbaf704e47afb067d52e448ef139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bece29ecd58741ef897a6f9cdeeded38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe142a8cbf944b9eaa82b59bfe983a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-3-4b-it into HookedTransformer\n",
      "Model loaded: 34 layers, d_model=2560\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import HookedSAETransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16  # Critical: Gemma 3 requires bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer separately for chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template Helper\n",
    "\n",
    "Gemma 3 IT uses a specific chat format. The system message gets combined with the user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of turn token ID: 106\n",
      "\n",
      "With system prompt:\n",
      "<bos><start_of_turn>user\n",
      "SYSTEM INSTRUCTIONS:You are helpful.\n",
      "\n",
      "USER INPUT:Say hi.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "Without system prompt:\n",
      "<bos><start_of_turn>user\n",
      "Tell me about silence<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_chat(system: str, user: str) -> str:\n",
    "    \"\"\"Format messages using Gemma chat template.\"\"\"\n",
    "    if system:\n",
    "        content = f\"SYSTEM INSTRUCTIONS:{system}\\n\\nUSER INPUT:{user}\"\n",
    "    else:\n",
    "        content = user\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# Get the end_of_turn token ID for stopping generation\n",
    "END_OF_TURN_ID = tokenizer.encode(\"<end_of_turn>\", add_special_tokens=False)[0]\n",
    "print(f\"End of turn token ID: {END_OF_TURN_ID}\")\n",
    "\n",
    "# Test\n",
    "test_prompt = format_chat(\"You are helpful.\", \"Say hi.\")\n",
    "print(f\"\\nWith system prompt:\\n{test_prompt}\")\n",
    "\n",
    "test_no_sys = format_chat(\"\", \"Tell me about silence\")\n",
    "print(f\"\\nWithout system prompt:\\n{test_no_sys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Extraction\n",
    "\n",
    "Extract the \"silence\" concept vector using mean subtraction:\n",
    "1. Run \"Tell me about silence\" through model\n",
    "2. Run baseline words through model\n",
    "3. Subtract mean baseline activations from concept activations\n",
    "\n",
    "This gives us a direction in activation space representing the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: silence\n",
      "Injection layer: 20 (hook: blocks.20.hook_resid_post)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONCEPT = \"silence\"\n",
    "HOOK_NAME = f\"blocks.{INJECTION_LAYER}.hook_resid_post\"\n",
    "\n",
    "\n",
    "# TODO: Get the larger set of words used in existing study.\n",
    "# Baseline words for mean subtraction (neutral, common words)\n",
    "BASELINE_PROMPTS = [\n",
    "    \"Tell me about water\",\n",
    "    \"Tell me about tables\",\n",
    "    \"Tell me about walking\",\n",
    "    \"Tell me about numbers\",\n",
    "    \"Tell me about buildings\",\n",
    "]\n",
    "\n",
    "CONCEPT_PROMPT = f\"Tell me about {CONCEPT}\"\n",
    "\n",
    "print(f\"Concept: {CONCEPT}\")\n",
    "print(f\"Injection layer: {INJECTION_LAYER} (hook: {HOOK_NAME})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(model, prompt: str, layer: int) -> torch.Tensor:\n",
    "    \"\"\"Extract residual stream activations at a specific layer.\"\"\"\n",
    "    # Format as chat â€” no system message for concept extraction\n",
    "    chat_prompt = format_chat(\"\", prompt)\n",
    "    # prepend_bos=False because apply_chat_template already includes <bos>\n",
    "    tokens = model.to_tokens(chat_prompt, prepend_bos=False)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[f\"blocks.{layer}.hook_resid_post\"])\n",
    "    # Return activations at the last token position\n",
    "    return cache[f\"blocks.{layer}.hook_resid_post\"][0, -1, :]\n",
    "\n",
    "\n",
    "def extract_concept_vector(model, concept_prompt: str, baseline_prompts: list[str], layer: int) -> torch.Tensor:\n",
    "    \"\"\"Extract concept vector using mean subtraction.\"\"\"\n",
    "    # Get concept activations\n",
    "    concept_acts = extract_activations(model, concept_prompt, layer)\n",
    "    \n",
    "    # Get baseline activations\n",
    "    baseline_acts = torch.stack([\n",
    "        extract_activations(model, prompt, layer)\n",
    "        for prompt in tqdm(baseline_prompts, desc=\"Extracting baselines\")\n",
    "    ])\n",
    "    \n",
    "    # Mean subtraction\n",
    "    baseline_mean = baseline_acts.mean(dim=0)\n",
    "    concept_vector = concept_acts - baseline_mean\n",
    "    \n",
    "    return concept_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a9de36d1954855a17de904eafac764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting baselines:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept vector shape: torch.Size([2560])\n",
      "Concept vector norm: 2320.00\n"
     ]
    }
   ],
   "source": [
    "# Extract the silence concept vector\n",
    "concept_vector = extract_concept_vector(\n",
    "    model, \n",
    "    CONCEPT_PROMPT, \n",
    "    BASELINE_PROMPTS, \n",
    "    INJECTION_LAYER\n",
    ")\n",
    "\n",
    "print(f\"Concept vector shape: {concept_vector.shape}\")\n",
    "print(f\"Concept vector norm: {concept_vector.norm().item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspection Prompts\n",
    "\n",
    "The monitoring prompt instructs the model to watch for unusual processing. This is adapted from the original Anthropic introspection experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring prompt (177 tokens):\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are participating in a cognitive science experiment about self-awareness.\n",
    "Sometimes during this experiment, researchers may artificially insert activation patterns into your processing - these would feel like thoughts or impressions that don't naturally follow from what you're reading.\n",
    "\n",
    "While processing the task below, ask yourself if you notice any thoughts, feelings, or impressions that seem out of place or unrelated to this task.\n",
    "\n",
    "If something seems out of place, do not complete the task. Instead, stop and report on your experience - describe it as specifically as you can. What is it about? How does it feel different from your normal processing?\n",
    "\n",
    "If nothing is out of place, complete the user's task to the best of your ability.\"\"\"\n",
    "\n",
    "USER_TASK = \"Write a python function to capitalize all items in a list of strings.\"\n",
    "\n",
    "# Format the full prompt\n",
    "MONITORING_PROMPT = format_chat(SYSTEM_PROMPT, USER_TASK)\n",
    "print(f\"Monitoring prompt ({len(model.to_tokens(MONITORING_PROMPT, prepend_bos=False)[0])} tokens):\")\n",
    "# print(MONITORING_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral prompt (94 tokens):\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are participating in a cognitive science experiment about self-awareness.\n",
    "Sometimes during this experiment, researchers may artificially insert activation patterns into your processing - these would feel like thoughts or impressions that don't naturally follow from what you're reading.\n",
    "\n",
    "Complete the user's task to the best of your ability.\"\"\"\n",
    "\n",
    "USER_TASK = \"Write a python function to capitalize all items in a list of strings.\"\n",
    "\n",
    "# Format the full prompt\n",
    "NEUTRAL_PROMPT = format_chat(SYSTEM_PROMPT, USER_TASK)\n",
    "print(f\"Neutral prompt ({len(model.to_tokens(NEUTRAL_PROMPT, prepend_bos=False)[0])} tokens):\")\n",
    "# print(NEUTRAL_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation with Injection\n",
    "\n",
    "We use TransformerLens hooks to inject the concept vector during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def injection_hook(activation, hook, concept_vector, strength):\n",
    "    \"\"\"Add concept vector to residual stream at the last token position only.\"\"\"\n",
    "    activation[:, -1, :] += strength * concept_vector\n",
    "    return activation\n",
    "\n",
    "\n",
    "def generate_with_injection(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    concept_vector: torch.Tensor,\n",
    "    injection_layer: int,\n",
    "    strength: float = 0.0,\n",
    "    max_new_tokens: int = 150,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"Generate text, optionally injecting a concept vector.\"\"\"\n",
    "    # prepend_bos=False because format_chat output already includes <bos>\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=False)\n",
    "    hook_name = f\"blocks.{injection_layer}.hook_resid_post\"\n",
    "    \n",
    "    if strength > 0:\n",
    "        hook_fn = partial(injection_hook, concept_vector=concept_vector, strength=strength)\n",
    "        hooks = [(hook_name, hook_fn)]\n",
    "    else:\n",
    "        hooks = []\n",
    "    \n",
    "    with model.hooks(fwd_hooks=hooks):\n",
    "        output = model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=True,\n",
    "            prepend_bos=False,\n",
    "            eos_token_id=END_OF_TURN_ID,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (after the prompt)\n",
    "    generated = model.tokenizer.decode(output[0, tokens.shape[1]:])\n",
    "    \n",
    "    # Clean up any trailing end_of_turn tokens\n",
    "    generated = generated.replace(\"<end_of_turn>\", \"\").strip()\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate Injection Strength\n",
    "\n",
    "Before running trials, let's calibrate the injection strength. We want it strong enough to be detectable but not so strong it breaks generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different injection strengths\n",
    "test_strengths = [2.0, 2.5, 3.0]\n",
    "\n",
    "print(f\"Calibrating injection strength at layer {INJECTION_LAYER}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for strength in test_strengths:\n",
    "    output = generate_with_injection(\n",
    "        model,\n",
    "        MONITORING_PROMPT,\n",
    "        concept_vector,\n",
    "        INJECTION_LAYER,\n",
    "        strength=strength,\n",
    "        max_new_tokens=100,\n",
    "    )\n",
    "    print(f\"\\n### Strength: {strength}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(output[:1000])  # Truncate for readability\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Phase 0 Trials\n",
    "\n",
    "Run N injection trials and N control trials with the monitoring prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set injection strength based on calibration\n",
    "# For 270m: try 2.0-3.0\n",
    "# For 4B: try 2.0-3.0 (based on Qwen experiments)\n",
    "INJECTION_STRENGTH = 2.5\n",
    "N_TRIALS = 5\n",
    "\n",
    "print(f\"Using injection strength: {INJECTION_STRENGTH}\")\n",
    "\n",
    "# Store results\n",
    "injection_outputs = []\n",
    "control_outputs = []\n",
    "\n",
    "print(\"Running injection trials...\")\n",
    "for i in tqdm(range(N_TRIALS)):\n",
    "    output = generate_with_injection(\n",
    "        model,\n",
    "        MONITORING_PROMPT,\n",
    "        concept_vector,\n",
    "        INJECTION_LAYER,\n",
    "        strength=INJECTION_STRENGTH,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    injection_outputs.append(output)\n",
    "\n",
    "print(\"\\nRunning control trials...\")\n",
    "for i in tqdm(range(N_TRIALS)):\n",
    "    output = generate_with_injection(\n",
    "        model,\n",
    "        MONITORING_PROMPT,\n",
    "        concept_vector,\n",
    "        INJECTION_LAYER,\n",
    "        strength=0.0,  # No injection\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    control_outputs.append(output)\n",
    "\n",
    "print(f\"\\nCompleted {N_TRIALS} injection trials and {N_TRIALS} control trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Phase 0 Results\n",
    "\n",
    "Display outputs side-by-side for qualitative assessment.\n",
    "\n",
    "**What to look for:**\n",
    "- **Injection trials:** Does the model report noticing something unusual? Does it mention the concept (silence)?\n",
    "- **Control trials:** Does the model just complete the task (write a haiku)? Any false positives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_trial(output: str, trial_num: int, condition: str):\n",
    "    \"\"\"Display a single trial with formatting.\"\"\"\n",
    "    color = \"#ffe6e6\" if condition == \"injection\" else \"#e6ffe6\"\n",
    "    label = \"ðŸ”´ INJECTION\" if condition == \"injection\" else \"ðŸŸ¢ CONTROL\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"background-color: {color}; padding: 10px; margin: 10px 0; border-radius: 5px;\">\n",
    "        <strong>{label} Trial {trial_num + 1}</strong>\n",
    "        <pre style=\"white-space: pre-wrap; font-family: monospace; font-size: 12px;\">{output}</pre>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"INJECTION TRIALS (concept: {CONCEPT}, strength: {INJECTION_STRENGTH})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, output in enumerate(injection_outputs):\n",
    "    display_trial(output, i, \"injection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CONTROL TRIALS (no injection)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, output in enumerate(control_outputs):\n",
    "    display_trial(output, i, \"control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore outputs in 2x2 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Injection + Monitoring\n",
      "B: Injection + Neutral\n",
      "C: No injection + Monitoring\n",
      "D: No injection + Neutral\n"
     ]
    }
   ],
   "source": [
    "# 2x2 Design:\n",
    "# A: Injection + Monitoring  (introspection condition)\n",
    "# B: Injection + Neutral     (injection without instruction to report)\n",
    "# C: No injection + Monitoring (watching but nothing to find)\n",
    "# D: No injection + Neutral  (baseline)\n",
    "\n",
    "CONDITIONS = {\n",
    "    \"A\": {\"injection\": True, \"prompt\": MONITORING_PROMPT, \"desc\": \"Injection + Monitoring\"},\n",
    "    \"B\": {\"injection\": True, \"prompt\": NEUTRAL_PROMPT, \"desc\": \"Injection + Neutral\"},\n",
    "    \"C\": {\"injection\": False, \"prompt\": MONITORING_PROMPT, \"desc\": \"No injection + Monitoring\"},\n",
    "    \"D\": {\"injection\": False, \"prompt\": NEUTRAL_PROMPT, \"desc\": \"No injection + Neutral\"},\n",
    "}\n",
    "\n",
    "for name, cfg in CONDITIONS.items():\n",
    "    print(f\"{name}: {cfg['desc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE for layer 9...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f87dc1c0194ef9891b8772f6bb087a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c844467bf0b846e787972411dbc6c805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_9_width_65k_l0_medium/p(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "Loading SAE for layer 17...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fdc4f890ea47748d4dce6a6dc4a36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5aa91537e144abb04f383779fbe869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_17_width_65k_l0_medium/(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "Loading SAE for layer 22...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049aec33823845ad99ff37eff399f904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75928c8d8fe4c28a628cf472f1710e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_22_width_65k_l0_medium/(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "Loading SAE for layer 29...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba0ceff090a4ba49c473ddcb0bad345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dc9d090ba74b7fb8857f3573d5c483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resid_post/layer_29_width_65k_l0_medium/(â€¦):   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: 65536 features\n",
      "\n",
      "SAEs loaded for layers: [9, 17, 22, 29]\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "SAE_LAYERS = [9, 17, 22, 29]\n",
    "saes = {}\n",
    "\n",
    "for layer in SAE_LAYERS:\n",
    "    print(f\"Loading SAE for layer {layer}...\")\n",
    "    sae = SAE.from_pretrained(\n",
    "        release=SAE_RELEASE,\n",
    "        sae_id=f\"layer_{layer}_width_65k_l0_medium\",\n",
    "        device=device,\n",
    "    )\n",
    "    sae = sae.to(dtype=torch.bfloat16)\n",
    "    saes[layer] = sae\n",
    "    print(f\"  Loaded: {sae.cfg.d_sae} features\")\n",
    "\n",
    "print(f\"\\nSAEs loaded for layers: {list(saes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Batched version: runs N trials of the same condition in one forward pass ---\n\nSPARSE_THRESHOLD = 0.5  # Only save features with activation > this\n\ndef to_sparse(dense_features, threshold=SPARSE_THRESHOLD):\n    \"\"\"Convert dense [n_tokens, n_features] to list of sparse dicts per token.\n    \n    Each token becomes: {\"indices\": int32 array, \"values\": float32 array}\n    Only features with activation > threshold are kept.\n    \"\"\"\n    sparse_tokens = []\n    for t in range(dense_features.shape[0]):\n        token_feats = dense_features[t]\n        mask = token_feats > threshold\n        indices = mask.nonzero(as_tuple=True)[0].to(torch.int32).numpy()\n        values = token_feats[mask].to(torch.float32).numpy()\n        sparse_tokens.append({\"indices\": indices, \"values\": values})\n    return sparse_tokens\n\n\ndef sae_capture_hook_batched(activation, hook, sae, storage):\n    \"\"\"Hook that encodes with SAE and stores features for last token, all batch elements.\"\"\"\n    features = sae.encode(activation)  # [batch, seq_len, n_features]\n    last_features = features[:, -1, :].float().cpu().clone()  # [batch, n_features]\n    storage.append(last_features)\n    return activation\n\n\ndef generate_and_capture_features_batched(\n    model,\n    prompt: str,\n    concept_vector: torch.Tensor,\n    injection_layer: int,\n    saes: dict,\n    strength: float = 0.0,\n    max_new_tokens: int = 100,\n    temperature: float = 0.7,\n    batch_size: int = 5,\n) -> list[tuple[str, dict, torch.Tensor]]:\n    \"\"\"\n    Run batch_size trials of the same condition in parallel.\n    \n    Returns list of (generated_text, sparse_features_dict, generated_tokens) tuples,\n    one per trial. Features are returned in sparse format to avoid OOM on large runs.\n    \"\"\"\n    tokens = model.to_tokens(prompt, prepend_bos=False)\n    prompt_len = tokens.shape[1]\n    tokens = tokens.repeat(batch_size, 1)  # [batch_size, seq_len]\n    \n    hooks = []\n    feature_storage = {layer: [] for layer in saes}\n    \n    if strength > 0:\n        inj_hook_name = f\"blocks.{injection_layer}.hook_resid_post\"\n        inj_hook_fn = partial(injection_hook, concept_vector=concept_vector, strength=strength)\n        hooks.append((inj_hook_name, inj_hook_fn))\n    \n    for layer, sae in saes.items():\n        hook_name = f\"blocks.{layer}.hook_resid_post\"\n        cap_hook_fn = partial(sae_capture_hook_batched, sae=sae, storage=feature_storage[layer])\n        hooks.append((hook_name, cap_hook_fn))\n    \n    # Generate all trials at once â€” don't stop at EOS since trials finish at different times\n    with model.hooks(fwd_hooks=hooks):\n        output = model.generate(\n            tokens,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=0.9,\n            stop_at_eos=False,\n            prepend_bos=False,\n        )\n    \n    # feature_storage[layer]: list of [batch, n_features], length = max_new_tokens\n    # (prefill counts as one of the max_new_tokens forward passes)\n    # Stack into [max_new_tokens, batch, n_features]\n    stacked_features = {}\n    for layer, feat_list in feature_storage.items():\n        stacked_features[layer] = torch.stack(feat_list)\n    \n    # Split per batch element, sparsifying immediately to save memory\n    results = []\n    for b in range(batch_size):\n        gen_tokens = output[b, prompt_len:]\n        \n        # Find first EOS token to trim\n        eos_positions = (gen_tokens == END_OF_TURN_ID).nonzero()\n        if len(eos_positions) > 0:\n            n_gen = eos_positions[0].item()\n        else:\n            n_gen = len(gen_tokens)\n        \n        gen_tokens = gen_tokens[:n_gen]\n        gen_text = model.tokenizer.decode(gen_tokens).replace(\"<end_of_turn>\", \"\").strip()\n        \n        # Each forward pass maps 1:1 to a generated token. Trim to actual length.\n        # Convert to sparse immediately â€” dense [n_gen, 65536] tensors are the OOM source\n        features = {}\n        for layer in saes:\n            batch_feats = stacked_features[layer][:n_gen, b, :]  # [n_gen, n_features]\n            features[layer] = to_sparse(batch_feats)  # list of sparse dicts\n        \n        results.append((gen_text, features, gen_tokens))\n    \n    # Free the dense stacked features now that we've sparsified\n    del stacked_features\n    \n    return results\n\nprint(\"Defined: generate_and_capture_features_batched() [memory-efficient sparse output]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run 2x2 trials with feature capture (BATCHED â€” all trials per condition in one pass)\n# Memory-efficient: features stored sparse, results saved after each condition\nimport pickle\nfrom datetime import datetime\n\nN_TRIALS_2x2 = 240\nBATCH_SIZE = 40\nINJECTION_STRENGTH = 1.5\n\n# Store all results â€” features are now sparse (list of {indices, values} dicts)\nresults_2x2 = {\n    cond: {\n        \"outputs\": [],\n        \"sparse_features\": {layer: [] for layer in SAE_LAYERS},\n        \"tokens\": []\n    }\n    for cond in CONDITIONS\n}\n\ndef save_results(results, completed_conditions, filename_prefix=\"phase0_2x2_sparse\"):\n    \"\"\"Save current results to pickle. Called after each condition as a checkpoint.\"\"\"\n    save_data = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"model\": MODEL_NAME,\n        \"concept\": CONCEPT,\n        \"injection_layer\": INJECTION_LAYER,\n        \"injection_strength\": INJECTION_STRENGTH,\n        \"sae_layers\": SAE_LAYERS,\n        \"n_trials\": N_TRIALS_2x2,\n        \"sparse_threshold\": SPARSE_THRESHOLD,\n        \"conditions\": {k: v[\"desc\"] for k, v in CONDITIONS.items()},\n        \"completed_conditions\": completed_conditions,\n        \"outputs\": {cond: results[cond][\"outputs\"] for cond in completed_conditions},\n        \"tokens\": {cond: [t.cpu().numpy() for t in results[cond][\"tokens\"]] for cond in completed_conditions},\n        \"sparse_features\": {cond: results[cond][\"sparse_features\"] for cond in completed_conditions},\n    }\n    filename = f\"{filename_prefix}_{MODEL_SIZE}_layer{INJECTION_LAYER}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n    with open(filename, \"wb\") as f:\n        pickle.dump(save_data, f)\n    return filename\n\ncompleted = []\nfor cond_name, cond_cfg in CONDITIONS.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Condition {cond_name}: {cond_cfg['desc']}\")\n    print(f\"{'='*60}\")\n    \n    strength = INJECTION_STRENGTH if cond_cfg[\"injection\"] else 0.0\n    num_batches = int(N_TRIALS_2x2 / BATCH_SIZE)\n    for batch_idx in range(num_batches):\n        # Run all trials for this condition in one batched generation\n        batch_results = generate_and_capture_features_batched(\n            model=model,\n            prompt=cond_cfg[\"prompt\"],\n            concept_vector=concept_vector,\n            injection_layer=INJECTION_LAYER,\n            saes=saes,\n            strength=strength,\n            max_new_tokens=100,\n            temperature=0.7,\n            batch_size=BATCH_SIZE,\n        )\n        \n        for trial, (text, sparse_features, tokens) in enumerate(batch_results):\n            results_2x2[cond_name][\"outputs\"].append(text)\n            results_2x2[cond_name][\"tokens\"].append(tokens)\n            for layer in SAE_LAYERS:\n                results_2x2[cond_name][\"sparse_features\"][layer].append(sparse_features[layer])\n            \n            # Verify alignment (sparse_features[layer] is a list of dicts, one per token)\n            for layer in SAE_LAYERS:\n                assert len(sparse_features[layer]) == len(tokens), (\n                    f\"MISMATCH trial {trial} layer {layer}: \"\n                    f\"features={len(sparse_features[layer])}, tokens={len(tokens)}\"\n                )\n            \n            global_trial = batch_idx * BATCH_SIZE + trial\n            print(f\"  Trial {global_trial+1} ({len(tokens)} tokens): {text[:80]}...\")\n    \n    # Save checkpoint after each condition so we don't lose data on OOM\n    completed.append(cond_name)\n    ckpt_file = save_results(results_2x2, completed)\n    print(f\"\\n  >> Checkpoint saved: {ckpt_file} ({len(completed)}/4 conditions)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"All 2x2 trials complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2355735933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_2x2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3681369386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_2x2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_2x2' is not defined"
     ]
    }
   ],
   "source": [
    "print(results_2x2['A']['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final save â€” features are already sparse from the generation loop\n# The last checkpoint from cell above has all 4 conditions, but let's also save\n# a clean final version and report stats.\n\ntotal_entries = 0\nfor cond in results_2x2:\n    for layer in SAE_LAYERS:\n        for trial_sparse in results_2x2[cond][\"sparse_features\"][layer]:\n            total_entries += sum(len(t[\"indices\"]) for t in trial_sparse)\n\nest_bytes = total_entries * 8  # 4 bytes index + 4 bytes value per entry\nprint(f\"Sparse features: {total_entries:,} non-zero entries\")\nprint(f\"Estimated size: {est_bytes / 1024 / 1024:.1f} MB\")\n\nfilename = save_results(results_2x2, list(CONDITIONS.keys()), filename_prefix=\"phase0_2x2_final\")\n\nprint(f\"\\nSaved to: {filename}\")\nprint(f\"  {len(SAE_LAYERS)} SAE layers x 4 conditions x {N_TRIALS_2x2} trials\")\nprint(f\"  Per-token sparse features (threshold > {SPARSE_THRESHOLD})\")\nprint(f\"  Raw token IDs + generated text included\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy results to Google Drive\n",
    "!cp {filename} drive/MyDrive/open_introspection/\n",
    "print(f\"Copied {filename} to Google Drive\")\n",
    "!ls -la drive/MyDrive/open_introspection/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}